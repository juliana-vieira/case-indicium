# üíö Case Indicium - Programa Lighthouse

## Descri√ß√£o do Projeto

Este reposit√≥rio cont√©m a solu√ß√£o para o case t√©cnico de Engenharia de Dados do Banco Vit√≥ria (BanVic), que simula a implementa√ß√£o de um pipeline de dados para centralizar informa√ß√µes de diferentes fontes em um Data Warehouse.

O projeto aborda a jornada de maturidade de dados do BanVic, focando inicialmente na an√°lise de dados de cr√©dito como projeto piloto para demonstrar valor para a organiza√ß√£o.

### Contexto do Desafio

O Banco Vit√≥ria S.A. (BanVic) busca evoluir sua cultura de dados atrav√©s da implementa√ß√£o de um pipeline de dados que centralize informa√ß√µes em um Data Warehouse. Atualmente, as an√°lises s√£o realizadas manualmente em planilhas, o que limita a capacidade de gera√ß√£o de insights para tomada de decis√£o.
  
---

## Arquitetura da Solu√ß√£o

```mermaid
graph LR
    A[CSV] --> C[Apache Airflow]
    B[SQL] --> C
    C --> D[FileSystem Local]
    D --> E[PostgreSQL DW]
    
    F[CSV + SQL] -.-> A
    G[Orquestra√ß√£o] -.-> C
    H[Staging Area] -.-> D
    I[Destino Final] -.-> E
    
    class A,B source
    class C orchestration
    class D storage
    class E warehouse
    class F,G,H,I legend
```

---

## Modelagem do Data Warehouse

```mermaid
erDiagram
    DIM_AGENCIAS ||--o{ DIM_COLABORADOR_AGENCIA : "cod_agencia"
    DIM_COLABORADORES ||--o{ DIM_COLABORADOR_AGENCIA : "cod_colaborador"
    DIM_COLABORADORES ||--o{ FATO_PROPOSTAS : "cod_colaborador"
    DIM_CLIENTES ||--o{ FATO_PROPOSTAS : "cod_cliente"
    FATO_CONTAS ||--o{ FATO_TRANSACOES : "num_conta"
    DIM_CONTAS ||--o{ FATO_TRANSACOES : "num_conta"
    DIM_CONTAS ||--o{ FATO_CONTAS : "num_conta"

    DIM_AGENCIAS {
        int cod_agencia PK
        string nome
        string endereco
        string cidade
        string uf
        date data_abertura
        string tipo_agencia
        timestamp ultima_atualizacao
    }

    DIM_COLABORADOR_AGENCIA {
        int cod_colaborador PK
        int cod_agencia FK
        timestamp ultima_atualizacao
    }

    DIM_COLABORADORES {
        int cod_colaborador PK
        string primeiro_nome
        string ultimo_nome
        string email
        string cpf
        date data_nascimento
        string endereco
        string cep
        timestamp ultima_atualizacao
    }

    FATO_PROPOSTAS {
        int cod_proposta PK
        int cod_cliente FK
        int cod_colaborador FK
        date data_entrada_proposta
        decimal taxa_juros_mensal
        decimal valor_proposta
        decimal valor_financiamento
        decimal valor_entrada
        decimal valor_prestacao
        int quantidade_parcelas
        string carencia
        string status_proposta
        timestamp ultima_atualizacao
    }

    DIM_CLIENTES {
        int cod_cliente PK
        string primeiro_nome
        string ultimo_nome
        string email
        string tipo_cliente
        date data_inclusao
        string cpfcnpj
        date data_nascimento
        string endereco
        string cep
        timestamp ultima_atualizacao
    }

    FATO_TRANSACOES {
        int cod_transacao PK
        int num_conta FK
        date data_transacao
        string nome_transacao
        decimal valor_transacao
        timestamp ultima_atualizacao
    }

    FATO_CONTAS {
        int num_conta PK
        decimal saldo_total
        decimal saldo_disponivel
        date data_ultimo_lancamento
        timestamp ultima_atualizacao
    }

    DIM_CONTAS {
        int num_conta PK
        string tipo_conta
        date data_abertura
        int cod_cliente FK
        int cod_agencia FK
        int cod_colaborador FK
        timestamp ultima_atualizacao
    }
```

---

### Funcionalidades Implementadas

- Orquestra√ß√£o com Apache Airflow: Pipeline gerenciado e agendado

- Extra√ß√£o Idempotente: Processos reprodut√≠veis e consistentes

- Extra√ß√£o em Paralelo: CSV e SQL processados simultaneamente

- Padroniza√ß√£o de Arquivos: Estrutura de diret√≥rios organizada

- Carregamento Condicional: DW s√≥ √© atualizado se ambas extra√ß√µes forem bem-sucedidas

- Agendamento Autom√°tico: Execu√ß√£o di√°ria √†s 04:35

- Ambiente Reproduz√≠vel: Configura√ß√£o via Docker Compose
  
---

## Limita√ß√µes Conhecidas

Este projeto foi desenvolvido para fins de **demonstra√ß√£o t√©cnica** e possui as seguintes limita√ß√µes:

### Data Lake Simulado
- Utiliza sistema de arquivos local para simular um Data Lake em vez de uma solu√ß√£o cloud (AWS S3, Azure Data Lake, etc.)

### Aus√™ncia de CDC (Change Data Capture)
- Os dados mais atualizados s√£o determinados de acordo com a data do diret√≥rio do Data Lake
- N√£o captura mudan√ßas em tempo real 
- Dados podem ficar desatualizados entre execu√ß√µes

### Ambiente de Desenvolvimento
- Configura√ß√£o simplificada para demonstra√ß√£o
- Execu√ß√£o em containers locais via Docker
- Sem pipeline CI/CD implementado
- Logs e monitoramento b√°sicos

---

## Melhorias Futuras

Para uma implementa√ß√£o em produ√ß√£o, seria necess√°rio:

1. **Implementar CDC** para captura de mudan√ßas em tempo real
2. **Migrar para Data Lake real** (AWS S3, Azure Data Lake, GCP Cloud Storage)
3. **Implementar data quality** e valida√ß√µes autom√°ticas robustas
4. **Configurar pipeline CI/CD** para automa√ß√£o de deploys
5. **Implementar data lineage** e cataloga√ß√£o de dados
6. **Adicionar testes automatizados** para os pipelines
7. **Configurar backup e recovery** dos dados
8. **Implementar seguran√ßa** e controle de acesso

---

## Contato

**Juliana Vieira**
- LinkedIn: [linkedin.com/in/juliana-vieira](https://linkedin.com/in/juliana-vieira)
- Email: julianasalustianovieira@gmail.com

---

*Projeto desenvolvido com üíö para o Programa Lighthouse 2025*
